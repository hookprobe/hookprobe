# AIOCHI - AI Eyes Container Stack
# Cognitive Network Layer for HookProbe Fortress
#
# Usage:
#   podman-compose -f podman-compose.aiochi.yml up -d
#
# Note: This stack is OPTIONAL. Core security (QSecBit, dnsXai) runs without it.
#       AIOCHI adds the "Eyes" - visual dashboards and human narratives.

name: aiochi

networks:
  aiochi-internal:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.210.0/24
          gateway: 172.20.210.1

volumes:
  clickhouse_data:
  clickhouse_logs:
  grafana_data:
  victoria_data:
  n8n_data:
  suricata_logs:
  zeek_logs:
  zeek_spool:
  identity_data:
  ollama_models:

services:
  # ============================================================================
  # DATA TIER - Event Storage
  # ============================================================================

  clickhouse:
    image: docker.io/clickhouse/clickhouse-server:24.8
    container_name: aiochi-clickhouse
    hostname: aiochi-clickhouse
    restart: unless-stopped
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.10
    ports:
      - "127.0.0.1:8123:8123"   # HTTP API (internal only)
      - "127.0.0.1:9000:9000"   # Native protocol (internal only)
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_logs:/var/log/clickhouse-server
      # Main AIOCHI schema (executed first - creates database and tables)
      - ../schemas/clickhouse-init.sql:/docker-entrypoint-initdb.d/00-schema.sql:ro
      # Additional initialization (executed after schema)
      - ./configs/clickhouse/init.sql:/docker-entrypoint-initdb.d/99-init.sql:ro
      - ./configs/clickhouse/users.xml:/etc/clickhouse-server/users.d/users.xml:ro
    environment:
      CLICKHOUSE_DB: aiochi
      CLICKHOUSE_USER: aiochi
      CLICKHOUSE_PASSWORD: "${CLICKHOUSE_PASSWORD:-aiochi_secure_password}"
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    ulimits:
      nofile:
        soft: 262144
        hard: 262144

  victoria:
    image: docker.io/victoriametrics/victoria-metrics:v1.106.1
    container_name: aiochi-victoria
    hostname: aiochi-victoria
    restart: unless-stopped
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.11
    ports:
      - "127.0.0.1:8428:8428"
    volumes:
      - victoria_data:/victoria-metrics-data
    command:
      - "--retentionPeriod=30d"
      - "--httpListenAddr=:8428"
      - "--storageDataPath=/victoria-metrics-data"
    healthcheck:
      test: ["CMD", "wget", "-q", "-O-", "http://localhost:8428/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================================================
  # CAPTURE TIER - Network Traffic Analysis
  # ============================================================================
  #
  # IMPORTANT: These containers capture from the OVS mirror port (FTS-mirror),
  # NOT the main FTS bridge. Direct AF_PACKET capture on an OVS bridge causes:
  #   1. Packet loss (competes with OVS datapath)
  #   2. Promiscuous mode conflicts
  #   3. CPU starvation
  #
  # The FTS-mirror port receives a copy of all traffic via OVS port mirroring,
  # which is the proper way to capture traffic for IDS/NSM analysis.
  # See: products/fortress/devices/common/ovs-post-setup.sh (setup_traffic_mirror)
  #
  # Default: CAPTURE_INTERFACE=FTS-mirror (OVS mirror port)
  # ============================================================================

  suricata:
    image: docker.io/jasonish/suricata:7.0.8
    container_name: aiochi-suricata
    hostname: aiochi-suricata
    restart: unless-stopped
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
      - SYS_NICE
    volumes:
      - suricata_logs:/var/log/suricata
      - ./configs/suricata/suricata.yaml:/etc/suricata/suricata.yaml:ro
      - ./configs/suricata/rules:/etc/suricata/rules:ro
    environment:
      SURICATA_OPTIONS: "-i ${CAPTURE_INTERFACE:-FTS-mirror} --af-packet"
    command: ["-c", "/etc/suricata/suricata.yaml", "-i", "${CAPTURE_INTERFACE:-FTS-mirror}", "--af-packet"]
    healthcheck:
      test: ["CMD", "pgrep", "-x", "suricata"]
      interval: 30s
      timeout: 10s
      retries: 3

  zeek:
    image: docker.io/zeek/zeek:7.0.3
    container_name: aiochi-zeek
    hostname: aiochi-zeek
    restart: unless-stopped
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    volumes:
      - zeek_logs:/opt/zeek/logs
      - zeek_spool:/opt/zeek/spool
      - ./configs/zeek/local.zeek:/opt/zeek/share/zeek/site/local.zeek:ro
      - ./configs/zeek/node.cfg:/opt/zeek/etc/node.cfg:ro
    environment:
      ZEEK_INTERFACE: "${CAPTURE_INTERFACE:-FTS-mirror}"
    command: ["zeekctl", "deploy"]
    healthcheck:
      test: ["CMD", "zeekctl", "status"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============================================================================
  # INTELLIGENCE TIER - Device Identity & Narratives
  # ============================================================================

  identity-engine:
    build:
      context: ..
      dockerfile: containers/Containerfile.identity
    container_name: aiochi-identity
    hostname: aiochi-identity
    restart: unless-stopped
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.20
    ports:
      - "127.0.0.1:8060:8060"
    volumes:
      - identity_data:/app/data
      - /var/lib/misc:/var/lib/misc:ro          # dnsmasq leases
      - /run/avahi-daemon:/run/avahi-daemon:ro  # mDNS socket
    environment:
      # Use container DNS name instead of hardcoded IP
      CLICKHOUSE_HOST: aiochi-clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_DB: aiochi
      CLICKHOUSE_USER: aiochi
      CLICKHOUSE_PASSWORD: "${CLICKHOUSE_PASSWORD:-aiochi_secure_password}"
      LOG_LEVEL: "${LOG_LEVEL:-INFO}"
    depends_on:
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8060/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================================================
  # BUBBLE MANAGER - Ecosystem Device Detection
  # ============================================================================
  # Same-user device detection via Zeek log analysis (NO host network!)
  # Reads mDNS/D2D data from Zeek logs instead of live capture
  # This eliminates port 5353 conflicts and network blocking
  bubble-manager:
    build:
      context: ..
      dockerfile: containers/Containerfile.bubble
    container_name: aiochi-bubble
    hostname: aiochi-bubble
    restart: unless-stopped
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.25
    volumes:
      # Zeek logs for mDNS and D2D connection analysis
      - zeek_logs:/opt/zeek/logs:ro
      # Persistent bubble state
      - /var/lib/hookprobe:/var/lib/hookprobe:U
      - /var/lib/fortress:/var/lib/fortress:U
      # OVS socket for OpenFlow rules (bubble policies)
      - /run/openvswitch:/run/openvswitch
      # Fortress configuration
      - /etc/hookprobe:/etc/hookprobe:ro
    environment:
      # Database connection (Fortress PostgreSQL)
      DATABASE_HOST: "${FORTRESS_DB_HOST:-172.20.200.10}"
      DATABASE_PORT: 5432
      DATABASE_NAME: fortress
      DATABASE_USER: fortress
      DATABASE_PASSWORD: "${POSTGRES_PASSWORD:-fortress_db_secret}"
      # ClickHouse for analytics
      CLICKHOUSE_HOST: aiochi-clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_DB: aiochi
      CLICKHOUSE_USER: aiochi
      CLICKHOUSE_PASSWORD: "${CLICKHOUSE_PASSWORD:-aiochi_secure_password}"
      # Zeek log paths
      ZEEK_LOG_PATH: /opt/zeek/logs/current
      ZEEK_DNS_LOG: /opt/zeek/logs/current/dns.log
      ZEEK_CONN_LOG: /opt/zeek/logs/current/conn.log
      # Logging
      LOG_LEVEL: "${LOG_LEVEL:-INFO}"
      PYTHONPATH: /opt/hookprobe
    cap_add:
      - NET_ADMIN  # Required for OVS OpenFlow commands
    depends_on:
      zeek:
        condition: service_started
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'bubble' > /dev/null || curl -sf http://localhost:8070/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  narrative-engine:
    image: docker.io/n8nio/n8n:1.70.3
    container_name: aiochi-narrative
    hostname: aiochi-narrative
    restart: unless-stopped
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.21
    ports:
      - "127.0.0.1:5678:5678"
    volumes:
      - n8n_data:/home/node/.n8n
      - ./n8n-workflows:/home/node/workflows:ro
    environment:
      N8N_BASIC_AUTH_ACTIVE: "true"
      N8N_BASIC_AUTH_USER: "${N8N_USER:-admin}"
      N8N_BASIC_AUTH_PASSWORD: "${N8N_PASSWORD:-aiochi_admin}"
      N8N_HOST: "0.0.0.0"
      N8N_PORT: "5678"
      N8N_PROTOCOL: "http"
      WEBHOOK_URL: "http://aiochi-narrative:5678"
      GENERIC_TIMEZONE: "${TZ:-UTC}"
      # ClickHouse connection (use container DNS names)
      CLICKHOUSE_HOST: "aiochi-clickhouse"
      CLICKHOUSE_PORT: "8123"
      CLICKHOUSE_DB: "aiochi"
      CLICKHOUSE_USER: "aiochi"
      CLICKHOUSE_PASSWORD: "${CLICKHOUSE_PASSWORD:-aiochi_secure_password}"
      # Ollama LLM connection (Agentic AI)
      OLLAMA_HOST: "http://aiochi-ollama:11434"
      OLLAMA_MODEL: "${OLLAMA_MODEL:-llama3.2:3b}"
      # Agentic mode (template fallback for speed)
      NARRATIVE_MODE: "${NARRATIVE_MODE:-agentic}"
      # n8n AI features
      N8N_AI_ENABLED: "true"
    depends_on:
      clickhouse:
        condition: service_healthy
      identity-engine:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5678/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================================================
  # VISUALIZATION TIER - Dashboard
  # ============================================================================

  grafana:
    image: docker.io/grafana/grafana:11.4.0
    container_name: aiochi-grafana
    hostname: aiochi-grafana
    restart: unless-stopped
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.30
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./configs/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./configs/grafana/dashboards:/var/lib/grafana/dashboards:ro
    environment:
      GF_SECURITY_ADMIN_USER: "${GRAFANA_USER:-admin}"
      GF_SECURITY_ADMIN_PASSWORD: "${GRAFANA_PASSWORD:-aiochi_admin}"
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      GF_SERVER_ROOT_URL: "${GRAFANA_ROOT_URL:-http://localhost:3000}"
      GF_INSTALL_PLUGINS: "grafana-clickhouse-datasource"
      # Dark theme by default
      GF_USERS_DEFAULT_THEME: "dark"
      # Disable alerting (we use our own)
      GF_ALERTING_ENABLED: "false"
      GF_UNIFIED_ALERTING_ENABLED: "false"
    depends_on:
      clickhouse:
        condition: service_healthy
      victoria:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================================================
  # LOG SHIPPER - ClickHouse Integration
  # ============================================================================

  log-shipper:
    build:
      context: ..
      dockerfile: containers/Containerfile.logshipper
    container_name: aiochi-logshipper
    hostname: aiochi-logshipper
    restart: unless-stopped
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.40
    volumes:
      - suricata_logs:/var/log/suricata:ro
      - zeek_logs:/opt/zeek/logs:ro
    environment:
      # Use container DNS name instead of hardcoded IP
      CLICKHOUSE_HOST: aiochi-clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_DB: aiochi
      CLICKHOUSE_USER: aiochi
      CLICKHOUSE_PASSWORD: "${CLICKHOUSE_PASSWORD:-aiochi_secure_password}"
      SURICATA_LOG_PATH: /var/log/suricata/eve.json
      ZEEK_LOG_PATH: /opt/zeek/logs/current
      LOG_LEVEL: "${LOG_LEVEL:-INFO}"
    depends_on:
      clickhouse:
        condition: service_healthy
      suricata:
        condition: service_started
      zeek:
        condition: service_started
    healthcheck:
      test: ["CMD", "pgrep", "-x", "python"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============================================================================
  # AI TIER - Local LLM for Agentic Reasoning
  # ============================================================================

  ollama:
    image: docker.io/ollama/ollama:latest
    container_name: aiochi-ollama
    hostname: aiochi-ollama
    restart: unless-stopped
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.50
    ports:
      - "127.0.0.1:11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      OLLAMA_HOST: "0.0.0.0"
      OLLAMA_KEEP_ALIVE: "24h"
      # GPU acceleration (optional - auto-detected)
      # NVIDIA_VISIBLE_DEVICES: all
    # Pull default model on first start (async - don't block container ready)
    # Model download (~2GB) happens in background, healthcheck waits for API
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        /bin/ollama serve &
        OLLAMA_PID=$!
        # Wait for Ollama API to be ready (up to 30s)
        for i in $(seq 1 30); do
          if curl -sf http://localhost:11434/api/tags >/dev/null 2>&1; then
            echo "Ollama API ready, pulling model in background..."
            # Pull model in background (don't block container startup)
            nohup /bin/ollama pull llama3.2:3b >/var/log/ollama-pull.log 2>&1 &
            break
          fi
          sleep 1
        done
        wait $OLLAMA_PID
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      # Extended start period for slow systems (model download is async now)
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
