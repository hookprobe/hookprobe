# AIOCHI - AI Eyes Container Stack
# Cognitive Network Layer for HookProbe Fortress
#
# Usage:
#   # Minimal (recommended - ~2GB RAM)
#   podman-compose -f podman-compose.aiochi.yml up -d
#
#   # With AI narratives (~6GB RAM)
#   podman-compose -f podman-compose.aiochi.yml --profile ai up -d
#
#   # With n8n workflows (~3GB RAM)
#   podman-compose -f podman-compose.aiochi.yml --profile workflows up -d
#
#   # Full stack (~8GB RAM)
#   podman-compose -f podman-compose.aiochi.yml --profile ai --profile workflows up -d
#
# Note: This stack is OPTIONAL. Core security (QSecBit, dnsXai) runs without it.
#       AIOCHI adds the "Eyes" - visual dashboards and human narratives.
#
# Minimal Stack (~2GB RAM):
#   - clickhouse: Event analytics database
#   - suricata: IDS threat detection
#   - zeek: Connection/protocol logging
#   - log-shipper: Data pipeline
#   - identity-engine: Device fingerprinting (AIOCHI core)
#   - bubble-manager: Ecosystem detection (AIOCHI core)
#
# Note: Visualization is handled by Fortress AdminLTE web UI (no Grafana needed)
#
# Optional (profile: ai):
#   - ollama: Local LLM for AI narratives (~4GB additional RAM)
#
# Optional (profile: workflows):
#   - narrative-engine (n8n): Complex workflow automation (~500MB additional RAM)

name: aiochi

networks:
  aiochi-internal:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.210.0/24
          gateway: 172.20.210.1

volumes:
  clickhouse_data:
  clickhouse_logs:
  n8n_data:
  suricata_logs:
  zeek_logs:
  zeek_spool:
  identity_data:
  ollama_models:
  bubble_data:

services:
  # ============================================================================
  # DATA TIER - Event Storage (CORE)
  # ============================================================================

  clickhouse:
    image: docker.io/clickhouse/clickhouse-server:24.8
    container_name: aiochi-clickhouse
    hostname: aiochi-clickhouse
    restart: unless-stopped
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.10
    ports:
      - "127.0.0.1:8123:8123"   # HTTP API (internal only)
      - "127.0.0.1:9000:9000"   # Native protocol (internal only)
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_logs:/var/log/clickhouse-server
      # Main AIOCHI schema (executed first - creates database and tables)
      - ../schemas/clickhouse-init.sql:/docker-entrypoint-initdb.d/00-schema.sql:ro
      # Additional initialization (executed after schema)
      - ./configs/clickhouse/init.sql:/docker-entrypoint-initdb.d/99-init.sql:ro
      - ./configs/clickhouse/users.xml:/etc/clickhouse-server/users.d/users.xml:ro
    environment:
      CLICKHOUSE_DB: aiochi
      CLICKHOUSE_USER: aiochi
      CLICKHOUSE_PASSWORD: "${CLICKHOUSE_PASSWORD:-aiochi_secure_password}"
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    ulimits:
      nofile:
        soft: 262144
        hard: 262144

  # ============================================================================
  # CAPTURE TIER - Network Traffic Analysis (CORE)
  # ============================================================================
  #
  # IMPORTANT: These containers capture from the OVS mirror port (FTS-mirror),
  # NOT the main FTS bridge. Direct AF_PACKET capture on an OVS bridge causes:
  #   1. Packet loss (competes with OVS datapath)
  #   2. Promiscuous mode conflicts
  #   3. CPU starvation
  #
  # The FTS-mirror port receives a copy of all traffic via OVS port mirroring,
  # which is the proper way to capture traffic for IDS/NSM analysis.
  # See: products/fortress/devices/common/ovs-post-setup.sh (setup_traffic_mirror)
  #
  # Default: CAPTURE_INTERFACE=FTS-mirror (OVS mirror port)
  # ============================================================================

  suricata:
    image: docker.io/jasonish/suricata:7.0.8
    container_name: aiochi-suricata
    hostname: aiochi-suricata
    restart: unless-stopped
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
      - SYS_NICE
    volumes:
      - suricata_logs:/var/log/suricata
      - ./configs/suricata/suricata.yaml:/etc/suricata/suricata.yaml:ro
      - ./configs/suricata/rules:/etc/suricata/rules:ro
    environment:
      SURICATA_OPTIONS: "-i ${CAPTURE_INTERFACE:-FTS-mirror} --af-packet"
    command: ["-c", "/etc/suricata/suricata.yaml", "-i", "${CAPTURE_INTERFACE:-FTS-mirror}", "--af-packet"]
    healthcheck:
      # Check Suricata process and verify eve.json is being written
      test: ["CMD-SHELL", "pgrep -x suricata && test -f /var/log/suricata/eve.json"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  zeek:
    image: docker.io/zeek/zeek:7.0.3
    container_name: aiochi-zeek
    hostname: aiochi-zeek
    restart: unless-stopped
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    volumes:
      - zeek_logs:/opt/zeek/logs
      - zeek_spool:/opt/zeek/spool
      - ./configs/zeek/local.zeek:/opt/zeek/share/zeek/site/local.zeek:ro
      - ./configs/zeek/node.cfg:/opt/zeek/etc/node.cfg:ro
    environment:
      ZEEK_INTERFACE: "${CAPTURE_INTERFACE:-FTS-mirror}"
    command: ["zeekctl", "deploy"]
    healthcheck:
      # Check Zeek status and verify logs are being generated
      test: ["CMD-SHELL", "zeekctl status | grep -q running"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s

  # ============================================================================
  # LOG SHIPPER - Data Pipeline (CORE)
  # ============================================================================

  log-shipper:
    build:
      context: ..
      dockerfile: containers/Containerfile.logshipper
    container_name: aiochi-logshipper
    hostname: aiochi-logshipper
    restart: unless-stopped
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.40
    volumes:
      - suricata_logs:/var/log/suricata:ro
      - zeek_logs:/opt/zeek/logs:ro
    environment:
      # Use container DNS name instead of hardcoded IP
      CLICKHOUSE_HOST: aiochi-clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_DB: aiochi
      CLICKHOUSE_USER: aiochi
      CLICKHOUSE_PASSWORD: "${CLICKHOUSE_PASSWORD:-aiochi_secure_password}"
      SURICATA_LOG_PATH: /var/log/suricata/eve.json
      ZEEK_LOG_PATH: /opt/zeek/logs/current
      LOG_LEVEL: "${LOG_LEVEL:-INFO}"
    depends_on:
      clickhouse:
        condition: service_healthy
      suricata:
        condition: service_started
      zeek:
        condition: service_started
    healthcheck:
      # Check Python process is running and can connect to ClickHouse
      test: ["CMD-SHELL", "pgrep -f 'log_shipper' > /dev/null || pgrep -x python"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ============================================================================
  # INTELLIGENCE TIER - Device Identity & Bubble Detection (CORE)
  # ============================================================================
  # These are the AIOCHI-specific components that provide the "cognitive" layer.
  # They read from Zeek logs (not live capture) to avoid network conflicts.
  # ============================================================================

  identity-engine:
    build:
      context: ..
      dockerfile: containers/Containerfile.identity
    container_name: aiochi-identity
    hostname: aiochi-identity
    restart: unless-stopped
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.20
    ports:
      - "127.0.0.1:8060:8060"
    volumes:
      - identity_data:/app/data
      - /var/lib/misc:/var/lib/misc:ro          # dnsmasq leases
      - /run/avahi-daemon:/run/avahi-daemon:ro  # mDNS socket
    environment:
      # Use container DNS name instead of hardcoded IP
      CLICKHOUSE_HOST: aiochi-clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_DB: aiochi
      CLICKHOUSE_USER: aiochi
      CLICKHOUSE_PASSWORD: "${CLICKHOUSE_PASSWORD:-aiochi_secure_password}"
      LOG_LEVEL: "${LOG_LEVEL:-INFO}"
    depends_on:
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8060/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  bubble-manager:
    build:
      context: ..
      dockerfile: containers/Containerfile.bubble
    container_name: aiochi-bubble
    hostname: aiochi-bubble
    restart: unless-stopped
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.25
    ports:
      - "127.0.0.1:8070:8070"
    volumes:
      # Zeek logs for mDNS and D2D connection analysis
      - zeek_logs:/opt/zeek/logs:ro
      # Persistent bubble state
      - bubble_data:/var/lib/aiochi
      # OVS socket for OpenFlow rules (bubble policies)
      - /run/openvswitch:/run/openvswitch
      # Fortress configuration
      - /etc/hookprobe:/etc/hookprobe:ro
    environment:
      # ClickHouse for analytics
      CLICKHOUSE_HOST: aiochi-clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_DB: aiochi
      CLICKHOUSE_USER: aiochi
      CLICKHOUSE_PASSWORD: "${CLICKHOUSE_PASSWORD:-aiochi_secure_password}"
      # Zeek log paths
      ZEEK_LOG_PATH: /opt/zeek/logs/current
      ZEEK_DNS_LOG: /opt/zeek/logs/current/dns.log
      ZEEK_CONN_LOG: /opt/zeek/logs/current/conn.log
      # Narrative engine (optional - uses templates if not available)
      NARRATIVE_ENGINE_URL: "http://aiochi-narrative:5678"
      OLLAMA_URL: "http://aiochi-ollama:11434"
      # Logging
      LOG_LEVEL: "${LOG_LEVEL:-INFO}"
    cap_add:
      - NET_ADMIN  # Required for OVS OpenFlow commands
    depends_on:
      zeek:
        condition: service_started
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8070/health || pgrep -f 'bubble'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ============================================================================
  # OPTIONAL: WORKFLOW ENGINE (profile: workflows)
  # ============================================================================
  # n8n for complex event workflows. Most use cases work fine with the
  # built-in narrative templates. Enable with: --profile workflows
  # ============================================================================

  narrative-engine:
    image: docker.io/n8nio/n8n:1.70.3
    container_name: aiochi-narrative
    hostname: aiochi-narrative
    restart: unless-stopped
    profiles:
      - workflows
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.21
    ports:
      - "127.0.0.1:5678:5678"
    volumes:
      - n8n_data:/home/node/.n8n
      - ../n8n-workflows:/home/node/workflows:ro
    environment:
      N8N_BASIC_AUTH_ACTIVE: "true"
      N8N_BASIC_AUTH_USER: "${N8N_USER:-admin}"
      N8N_BASIC_AUTH_PASSWORD: "${N8N_PASSWORD:-aiochi_admin}"
      N8N_HOST: "0.0.0.0"
      N8N_PORT: "5678"
      N8N_PROTOCOL: "http"
      WEBHOOK_URL: "http://aiochi-narrative:5678"
      GENERIC_TIMEZONE: "${TZ:-UTC}"
      # ClickHouse connection (use container DNS names)
      CLICKHOUSE_HOST: "aiochi-clickhouse"
      CLICKHOUSE_PORT: "8123"
      CLICKHOUSE_DB: "aiochi"
      CLICKHOUSE_USER: "aiochi"
      CLICKHOUSE_PASSWORD: "${CLICKHOUSE_PASSWORD:-aiochi_secure_password}"
      # Ollama LLM connection (if available)
      OLLAMA_HOST: "http://aiochi-ollama:11434"
      OLLAMA_MODEL: "${OLLAMA_MODEL:-llama3.2:3b}"
      # n8n AI features
      N8N_AI_ENABLED: "true"
    depends_on:
      clickhouse:
        condition: service_healthy
      identity-engine:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5678/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # ============================================================================
  # OPTIONAL: AI TIER - Local LLM (profile: ai)
  # ============================================================================
  # Ollama for AI-generated narratives. Most events use templates which work
  # without this. Enable with: --profile ai
  # WARNING: Requires 4-8GB RAM for the LLM model
  # ============================================================================

  ollama:
    image: docker.io/ollama/ollama:latest
    container_name: aiochi-ollama
    hostname: aiochi-ollama
    restart: unless-stopped
    profiles:
      - ai
    networks:
      aiochi-internal:
        ipv4_address: 172.20.210.50
    ports:
      - "127.0.0.1:11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      OLLAMA_HOST: "0.0.0.0"
      OLLAMA_KEEP_ALIVE: "24h"
      # GPU acceleration (optional - auto-detected)
      # NVIDIA_VISIBLE_DEVICES: all
    # Pull default model on first start (async - don't block container ready)
    # Model download (~2GB) happens in background, healthcheck waits for API
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        /bin/ollama serve &
        OLLAMA_PID=$!
        # Wait for Ollama API to be ready (up to 30s)
        for i in $(seq 1 30); do
          if curl -sf http://localhost:11434/api/tags >/dev/null 2>&1; then
            echo "Ollama API ready, pulling model in background..."
            # Pull model in background (don't block container startup)
            nohup /bin/ollama pull llama3.2:3b >/var/log/ollama-pull.log 2>&1 &
            break
          fi
          sleep 1
        done
        wait $OLLAMA_PID
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      # Extended start period for slow systems (model download is async now)
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
