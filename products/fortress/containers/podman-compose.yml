# HookProbe Fortress - Container Orchestration with OVS
#
# Self-contained, modular deployment with OVS-based network security.
# All services run in isolation with OpenFlow-controlled traffic.
#
# Architecture:
#   - OVS bridge (FTS) provides OpenFlow-based tier isolation
#   - Podman bridge networks for container-to-container communication
#   - veth pairs connect containers to OVS for flow monitoring
#   - sFlow/IPFIX export to QSecBit for ML analysis
#   - Traffic mirroring captures all packets for threat detection
#
# Usage:
#   podman-compose up -d                       # Start core services
#   podman-compose --profile monitoring up -d  # Start with Grafana/Victoria
#   podman-compose --profile training run lstm # Run LSTM training job
#   podman-compose down                        # Stop all services
#   podman-compose logs -f web                 # Follow web container logs
#
# After startup, connect containers to OVS:
#   /opt/hookprobe/fortress/devices/common/ovs-container-network.sh connect-containers
#
# Container Groups:
#   - Core: postgres, redis, web (always running)
#   - Security: qsecbit-agent, dnsxai, dfs-intelligence (always running)
#   - Monitoring: grafana, victoria (--profile monitoring)
#   - Jobs: lstm-trainer (--profile training, one-shot)
#
# Network Security Tiers (via OVS OpenFlow):
#   - fts-data:     172.20.200.0/24 (postgres, redis) - NO internet
#   - fts-services: 172.20.201.0/24 (web, dnsxai, dfs) - internet OK
#   - fts-ml:       172.20.202.0/24 (lstm-trainer) - NO internet
#   - fts-mgmt:     172.20.203.0/24 (grafana, victoria) - NO internet
#   - fts-lan:      10.200.0.0/23-29 (WiFi/LAN clients, configurable) - NAT to internet
#
# OVS Security Features:
#   - OpenFlow 1.3+ tier isolation rules
#   - Traffic mirroring to fts-mirror port
#   - sFlow export to 127.0.0.1:6343
#   - IPFIX export to 127.0.0.1:4739
#   - QoS meters for rate limiting
#   - VXLAN tunnels for mesh connectivity
#
# Version: 5.4.0
# License: AGPL-3.0

version: '3.8'

services:
  # ============================================================
  # DATA TIER - Isolated Database Layer (fts-data)
  # ============================================================

  # PostgreSQL Database - Most sensitive, fully isolated in data tier
  postgres:
    image: docker.io/library/postgres:15-alpine
    container_name: fts-postgres
    restart: unless-stopped
    hostname: fts-postgres
    environment:
      POSTGRES_DB: fortress
      POSTGRES_USER: fortress
      # Use environment variable directly (podman secrets have permission issues with non-root users)
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD:-fortress_db_secret}"
      # Enable SSL (certificates mounted from volume)
      PGSSLMODE: prefer
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
      - postgres_certs:/certs:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U fortress -d fortress"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    # Data tier - fully isolated, no internet access
    networks:
      fts-data:
        ipv4_address: 172.20.200.10

  # Redis Cache (Sessions + Rate Limiting)
  redis:
    image: docker.io/library/redis:7-alpine
    container_name: fts-redis
    restart: unless-stopped
    hostname: fts-redis
    command: >
      redis-server
      --appendonly yes
      --maxmemory 128mb
      --maxmemory-policy allkeys-lru
      --bind 0.0.0.0
      --requirepass "${REDIS_PASSWORD:-fortress_redis_secret}"
    volumes:
      - redis_data:/data
    healthcheck:
      # Use shell form for proper env var expansion
      test: ["CMD-SHELL", "redis-cli -a $${REDIS_PASSWORD:-fortress_redis_secret} ping | grep -q PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    # Data tier - fully isolated, no internet access
    networks:
      fts-data:
        ipv4_address: 172.20.200.11

  # ============================================================
  # SERVICES TIER - Web and DNS (fts-services)
  # ============================================================

  # Fortress Web Application (Flask + Gunicorn)
  # Services tier with access to data tier for database connectivity
  web:
    build:
      context: ..
      dockerfile: containers/Containerfile.web
    image: localhost/fts-web:latest
    container_name: fts-web
    restart: unless-stopped
    hostname: fts-web
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      # Database connection via container DNS name (data tier)
      DATABASE_HOST: fts-postgres
      DATABASE_PORT: 5432
      DATABASE_NAME: fortress
      DATABASE_USER: fortress
      DATABASE_PASSWORD: "${POSTGRES_PASSWORD:-fortress_db_secret}"
      # Redis connection via container DNS name (data tier)
      REDIS_HOST: fts-redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: "${REDIS_PASSWORD:-fortress_redis_secret}"
      # Flask settings
      FLASK_ENV: production
      FLASK_SECRET_KEY: "${FLASK_SECRET_KEY:-fortress_flask_secret_key_change_me}"
      # Gunicorn settings
      GUNICORN_WORKERS: 2
      GUNICORN_THREADS: 4
      GUNICORN_BIND: "0.0.0.0:${WEB_PORT:-8443}"
      # ML service endpoints - QSecBit on host, others via container DNS
      QSECBIT_API_URL: http://172.20.201.1:9090
      DNSXAI_API_URL: http://fts-dnsxai:5353
      DFS_API_URL: http://fts-dfs:8050
      # Monitoring endpoints (mgmt tier via gateway)
      VICTORIA_URL: http://172.20.203.11:8428
    volumes:
      - web_data:/app/data
      - web_logs:/app/logs
      # Bind mount host config directory (install script creates users.json here)
      - /etc/hookprobe:/etc/hookprobe:ro
    # Web exposed on services tier, but also connected to data tier for DB access
    ports:
      - "8443:8443"
    networks:
      fts-services:
        ipv4_address: 172.20.201.10
      fts-data:
        # Secondary interface for database access
        ipv4_address: 172.20.200.20
    # Health check - wait up to 90s for entrypoint (DB wait up to 60s + app startup)
    healthcheck:
      test: ["CMD", "curl", "-f", "-k", "https://localhost:8443/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s

  # dnsXai Engine - DNS ML Protection
  dnsxai:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.dnsxai
    image: localhost/fts-dnsxai:latest
    container_name: fts-dnsxai
    restart: unless-stopped
    hostname: fts-dnsxai
    environment:
      DNSXAI_UPSTREAM: "1.1.1.1"
      DNSXAI_PROTECTION_LEVEL: 3
      # Path must match Containerfile: /opt/hookprobe/shared/dnsXai/data
      DNSXAI_DATA_DIR: /opt/hookprobe/shared/dnsXai/data
    volumes:
      # Mount over shared path to persist data while keeping bundled whitelist
      - dnsxai_data:/opt/hookprobe/shared/dnsXai/data
      - dnsxai_blocklists:/opt/hookprobe/shared/dnsXai/blocklists
      - /etc/hookprobe:/etc/hookprobe:ro
    ports:
      - "5353:5353/udp"
    healthcheck:
      # DNS query check - send query and verify response (no pgrep in slim images)
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(socket.AF_INET,socket.SOCK_DGRAM); s.settimeout(3); q=b'\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x09localhost\\x00\\x00\\x01\\x00\\x01'; s.sendto(q,('127.0.0.1',5353)); r=s.recv(512); s.close(); exit(0 if len(r)>12 else 1)"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    networks:
      fts-services:
        ipv4_address: 172.20.201.11
    # Core service - always started (provides mesh DNS protection)

  # DFS Intelligence - WiFi Channel ML
  dfs-intelligence:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.dfs
    image: localhost/fts-dfs:latest
    container_name: fts-dfs
    restart: unless-stopped
    hostname: fts-dfs
    environment:
      DFS_DATA_DIR: /opt/hookprobe/wireless/data
      DFS_DB_PATH: /opt/hookprobe/wireless/data/dfs_radar.db
    volumes:
      - dfs_data:/opt/hookprobe/wireless/data
      - /etc/hookprobe:/etc/hookprobe:ro
    ports:
      - "8050:8050"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8050/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      fts-services:
        ipv4_address: 172.20.201.12
    # Core service - always started (provides WiFi DFS intelligence)

  # ============================================================
  # SECURITY TIER - Threat Detection (fts-ml)
  # ============================================================

  # QSecBit Agent - Threat Detection (numpy, scipy, sklearn)
  # Uses host network for traffic capture on all interfaces
  qsecbit-agent:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.agent
    image: localhost/fts-agent:latest
    container_name: fts-qsecbit
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      QSECBIT_MODE: fortress
      # Database via container network gateway (host can reach container IPs)
      DATABASE_HOST: 172.20.200.10
      DATABASE_PORT: 5432
      DATABASE_NAME: fortress
      DATABASE_USER: fortress
      DATABASE_PASSWORD: "${POSTGRES_PASSWORD:-fortress_db_secret}"
      REDIS_HOST: 172.20.200.11
      REDIS_PORT: 6379
      REDIS_PASSWORD: "${REDIS_PASSWORD:-fortress_redis_secret}"
      QSECBIT_DATA_DIR: /opt/hookprobe/fortress/data
      # Metrics export to Victoria (mgmt tier)
      VICTORIA_PUSH_URL: http://172.20.203.11:8428/api/v1/write
    volumes:
      - agent_data:/opt/hookprobe/fortress/data
      - /etc/hookprobe:/etc/hookprobe:ro
      - /sys/class/powercap:/sys/class/powercap:ro  # RAPL energy monitoring
    # Host network REQUIRED for traffic analysis on all interfaces
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    # Health check - verify HTTP API is responding on port 9090
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:9090/health', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    # Core service - always started (provides mesh threat detection)

  # LSTM Trainer - PyTorch model training (scheduled job)
  lstm-trainer:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.lstm
    image: localhost/fts-lstm:latest
    container_name: fts-lstm-trainer
    environment:
      LSTM_MODEL_DIR: /opt/hookprobe/fortress/data/ml-models/trained
      LSTM_CHECKPOINT_DIR: /opt/hookprobe/fortress/data/ml-models/checkpoints
    volumes:
      - ml_models:/opt/hookprobe/fortress/data/ml-models
      - agent_data:/opt/hookprobe/fortress/data:ro  # Read threat data
    networks:
      fts-ml:
        ipv4_address: 172.20.202.10
    profiles:
      - training
    # One-shot job - doesn't restart
    restart: "no"

  # ============================================================
  # MANAGEMENT TIER - Monitoring (fts-mgmt)
  # ============================================================

  # Grafana - Visualization Dashboard
  grafana:
    image: docker.io/grafana/grafana:11.4.0
    container_name: fts-grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: "${GRAFANA_PASSWORD:-fortress_grafana_admin}"
      # Disable plugin installation - requires internet which may not be available in container network
      # GF_INSTALL_PLUGINS: grafana-clock-panel
      GF_SERVER_ROOT_URL: "%(protocol)s://%(domain)s:%(http_port)s/grafana/"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    ports:
      - "3000:3000"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      fts-mgmt:
        ipv4_address: 172.20.203.10
    profiles:
      - monitoring

  # VictoriaMetrics - Time Series Database
  victoria:
    image: docker.io/victoriametrics/victoria-metrics:v1.106.1
    container_name: fts-victoria
    restart: unless-stopped
    command:
      - "-storageDataPath=/storage"
      - "-retentionPeriod=30d"
      - "-httpListenAddr=:8428"
    volumes:
      - victoria_data:/storage
    ports:
      - "8428:8428"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8428/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      fts-mgmt:
        ipv4_address: 172.20.203.11
    profiles:
      - monitoring

# ============================================================
# SECRETS
# ============================================================
# NOTE: File-based secrets disabled due to podman permission issues with non-root users.
# Secrets are now passed via environment variables. For production, consider:
# - Using podman secret with proper UID mapping
# - Or using a secrets management solution like HashiCorp Vault
#
# secrets:
#   postgres_password:
#     file: ./secrets/postgres_password

# ============================================================
# VOLUMES (persistent storage)
# ============================================================
volumes:
  # Data Tier
  postgres_data:
    driver: local
    name: fts-postgres-data
  postgres_certs:
    driver: local
    name: fts-postgres-certs
  redis_data:
    driver: local
    name: fts-redis-data

  # Services Tier
  web_data:
    driver: local
    name: fts-web-data
  web_logs:
    driver: local
    name: fts-web-logs
  dnsxai_data:
    driver: local
    name: fts-dnsxai-data
  dnsxai_blocklists:
    driver: local
    name: fts-dnsxai-blocklists
  dfs_data:
    driver: local
    name: fts-dfs-data

  # ML Tier
  agent_data:
    driver: local
    name: fts-agent-data
  ml_models:
    driver: local
    name: fts-ml-models

  # Management Tier
  grafana_data:
    driver: local
    name: fts-grafana-data
  victoria_data:
    driver: local
    name: fts-victoria-data

  # Note: /etc/hookprobe is bind-mounted from host, not a named volume
  # This allows the install script to create users.json in the host directory

# ============================================================
# NETWORKS - OVS-Integrated 4-Tier Security Architecture
# ============================================================
#
# These Podman bridge networks provide basic container connectivity.
# The OVS bridge (FTS) provides the security layer:
#
#   ┌─────────────────────────────────────────────────────────┐
#   │                  OVS Bridge: FTS                      │
#   │  ┌─────────────┬─────────────┬──────────┬────────────┐ │
#   │  │FTS-data   │FTS-svc    │FTS-ml  │FTS-mgmt  │ │
#   │  │(internal)   │(internet OK)│(internal) │(internal)  │ │
#   │  └──────┬──────┴──────┬──────┴─────┬─────┴──────┬─────┘ │
#   │         │             │            │            │       │
#   │    ┌────┴────┐   ┌────┴────┐  ┌────┴────┐ ┌─────┴────┐ │
#   │    │postgres │   │  web    │  │  lstm   │ │ grafana  │ │
#   │    │redis    │   │dnsxai   │  │         │ │ victoria │ │
#   │    └─────────┘   │dfs      │  └─────────┘ └──────────┘ │
#   │                  └─────────┘                           │
#   │  ┌───────────────────────────────────────────────────┐ │
#   │  │ fts-lan (10.200.0.0/23-29) - WiFi/LAN clients │ │
#   │  └───────────────────────────────────────────────────┘ │
#   │                                                        │
#   │  OpenFlow Rules:                                       │
#   │  - Table 0:  Ingress classification (ARP, DHCP, DNS)   │
#   │  - Table 10: Tier isolation (drop cross-tier traffic)  │
#   │  - Table 20: Internet control (allow/deny per tier)    │
#   │  - Table 30: Mirror (copy to QSecBit)                  │
#   │  - Table 40: Output (L2 forwarding)                    │
#   └─────────────────────────────────────────────────────────┘
#
networks:
  # Tier 1: DATA - Most sensitive (database, cache)
  # OVS port: fts-data (172.20.200.1/24)
  # OpenFlow: NO internet, only web can reach this tier
  fts-data:
    driver: bridge
    internal: true  # NO internet access
    ipam:
      config:
        - subnet: 172.20.200.0/24
          gateway: 172.20.200.1
    driver_opts:
      mtu: "9000"  # Jumbo frames for DB traffic
      # Note: Docker's enable_icc is not needed - Podman enables ICC by default

  # Tier 2: SERVICES - Web, DNS, APIs
  # OVS port: fts-services (172.20.201.1/24)
  # OpenFlow: Internet OK (dnsXai needs upstream DNS)
  fts-services:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.201.0/24
          gateway: 172.20.201.1
    driver_opts:
      mtu: "1500"

  # Tier 3: ML - Threat detection, inference
  # OVS port: fts-ml (172.20.202.1/24)
  # OpenFlow: NO internet, QSecBit uses host network
  fts-ml:
    driver: bridge
    internal: true  # NO internet access
    ipam:
      config:
        - subnet: 172.20.202.0/24
          gateway: 172.20.202.1

  # Tier 4: MANAGEMENT - Monitoring, metrics
  # OVS port: fts-mgmt (172.20.203.1/24)
  # OpenFlow: NO internet, web can query metrics
  fts-mgmt:
    driver: bridge
    internal: true  # NO internet access
    ipam:
      config:
        - subnet: 172.20.203.0/24
          gateway: 172.20.203.1
