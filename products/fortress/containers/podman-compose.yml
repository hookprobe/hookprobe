# HookProbe Fortress - Container Orchestration with OVS
#
# Self-contained, modular deployment with OVS-based network security.
# All services run in isolation with OpenFlow-controlled traffic.
#
# Architecture:
#   - OVS bridge (43ess) provides OpenFlow-based tier isolation
#   - Podman bridge networks for container-to-container communication
#   - veth pairs connect containers to OVS for flow monitoring
#   - sFlow/IPFIX export to QSecBit for ML analysis
#   - Traffic mirroring captures all packets for threat detection
#
# Usage:
#   podman-compose up -d                       # Start core services
#   podman-compose --profile monitoring up -d  # Start with Grafana/Victoria
#   podman-compose --profile training run lstm # Run LSTM training job
#   podman-compose down                        # Stop all services
#   podman-compose logs -f web                 # Follow web container logs
#
# After startup, connect containers to OVS:
#   /opt/hookprobe/fortress/devices/common/ovs-container-network.sh connect-containers
#
# Container Groups:
#   - Core: postgres, redis, web (always running)
#   - Security: qsecbit-agent, dnsxai, dfs-intelligence (always running)
#   - Monitoring: grafana, victoria (--profile monitoring)
#   - Jobs: lstm-trainer (--profile training, one-shot)
#
# Network Security Tiers (via OVS OpenFlow):
#   - fortress-data:     172.20.200.0/24 (postgres, redis) - NO internet
#   - fortress-services: 172.20.201.0/24 (web, dnsxai, dfs) - internet OK
#   - fortress-ml:       172.20.202.0/24 (lstm-trainer) - NO internet
#   - fortress-mgmt:     172.20.203.0/24 (grafana, victoria) - NO internet
#   - fortress-lan:      10.200.0.0/23-29 (WiFi/LAN clients, configurable) - NAT to internet
#
# OVS Security Features:
#   - OpenFlow 1.3+ tier isolation rules
#   - Traffic mirroring to fortress-mirror port
#   - sFlow export to 127.0.0.1:6343
#   - IPFIX export to 127.0.0.1:4739
#   - QoS meters for rate limiting
#   - VXLAN tunnels for mesh connectivity
#
# Version: 5.4.0
# License: AGPL-3.0

version: '3.8'

services:
  # ============================================================
  # DATA TIER - Isolated Database Layer (fortress-data)
  # ============================================================

  # PostgreSQL Database - Most sensitive, fully isolated
  postgres:
    image: docker.io/library/postgres:15-alpine
    container_name: fortress-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: fortress
      POSTGRES_USER: fortress
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
      # Enable SSL (certificates mounted from volume)
      PGSSLMODE: prefer
    secrets:
      - postgres_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
      - postgres_certs:/certs:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U fortress -d fortress"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      fortress-data:
        ipv4_address: 172.20.200.10

  # Redis Cache (Sessions + Rate Limiting)
  redis:
    image: docker.io/library/redis:7-alpine
    container_name: fortress-redis
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --maxmemory 128mb
      --maxmemory-policy allkeys-lru
      --requirepass "${REDIS_PASSWORD:-fortress_redis_secret}"
    volumes:
      - redis_data:/data
    healthcheck:
      # Use shell form for proper env var expansion
      test: ["CMD-SHELL", "redis-cli -a $${REDIS_PASSWORD:-fortress_redis_secret} ping | grep -q PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    ports:
      - "6379:6379"
    networks:
      fortress-data:
        ipv4_address: 172.20.200.11

  # ============================================================
  # SERVICES TIER - Web and DNS (fortress-services)
  # ============================================================

  # Fortress Web Application (Flask + Gunicorn)
  web:
    build:
      context: ..
      dockerfile: containers/Containerfile.web
    image: localhost/fortress-web:latest
    container_name: fortress-web
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      # Database connection (via data network)
      DATABASE_HOST: 172.20.200.10
      DATABASE_PORT: 5432
      DATABASE_NAME: fortress
      DATABASE_USER: fortress
      DATABASE_PASSWORD_FILE: /run/secrets/postgres_password
      # Redis connection (via data network)
      REDIS_HOST: 172.20.200.11
      REDIS_PORT: 6379
      REDIS_PASSWORD: "${REDIS_PASSWORD:-fortress_redis_secret}"
      # Flask settings
      FLASK_ENV: production
      FLASK_SECRET_KEY_FILE: /run/secrets/flask_secret
      # Gunicorn settings
      GUNICORN_WORKERS: 2
      GUNICORN_THREADS: 4
      # ML service endpoints (via services network)
      QSECBIT_API_URL: http://localhost:9090
      DNSXAI_API_URL: http://172.20.201.11:5353
      DFS_API_URL: http://172.20.201.12:8050
      # Monitoring endpoints (via mgmt network)
      VICTORIA_URL: http://172.20.203.11:8428
    secrets:
      - postgres_password
      - flask_secret
    volumes:
      - web_data:/app/data
      - web_logs:/app/logs
      - fortress_config:/etc/hookprobe:ro
    ports:
      - "8443:8443"
    healthcheck:
      test: ["CMD", "curl", "-f", "-k", "https://localhost:8443/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    networks:
      fortress-data:
        ipv4_address: 172.20.200.20
      fortress-services:
        ipv4_address: 172.20.201.10
      fortress-ml:
        ipv4_address: 172.20.202.20
      fortress-mgmt:
        ipv4_address: 172.20.203.20

  # dnsXai Engine - DNS ML Protection
  dnsxai:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.dnsxai
    image: localhost/fortress-dnsxai:latest
    container_name: fortress-dnsxai
    restart: unless-stopped
    environment:
      DNSXAI_UPSTREAM: "1.1.1.1"
      DNSXAI_PROTECTION_LEVEL: 3
      DNSXAI_DATA_DIR: /opt/hookprobe/dnsXai/data
    volumes:
      - dnsxai_data:/opt/hookprobe/dnsXai/data
      - dnsxai_blocklists:/opt/hookprobe/dnsXai/blocklists
      - fortress_config:/etc/hookprobe:ro
    ports:
      - "5353:5353/udp"
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(socket.AF_INET, socket.SOCK_DGRAM); s.settimeout(2); s.sendto(b'test', ('localhost', 5353))"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      fortress-services:
        ipv4_address: 172.20.201.11
    # Core service - always started (provides mesh DNS protection)

  # DFS Intelligence - WiFi Channel ML
  dfs-intelligence:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.dfs
    image: localhost/fortress-dfs:latest
    container_name: fortress-dfs
    restart: unless-stopped
    environment:
      DFS_DATA_DIR: /opt/hookprobe/wireless/data
      DFS_DB_PATH: /opt/hookprobe/wireless/data/dfs_radar.db
    volumes:
      - dfs_data:/opt/hookprobe/wireless/data
      - fortress_config:/etc/hookprobe:ro
    ports:
      - "8050:8050"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8050/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      fortress-services:
        ipv4_address: 172.20.201.12
    # Core service - always started (provides WiFi DFS intelligence)

  # ============================================================
  # SECURITY TIER - Threat Detection (fortress-ml)
  # ============================================================

  # QSecBit Agent - Threat Detection (numpy, scipy, sklearn)
  # Uses host network for traffic capture
  qsecbit-agent:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.agent
    image: localhost/fortress-agent:latest
    container_name: fortress-qsecbit
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      QSECBIT_MODE: fortress
      # Database via host network (postgres exposed via port forward)
      DATABASE_HOST: localhost
      DATABASE_PORT: 5432
      DATABASE_NAME: fortress
      DATABASE_USER: fortress
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      REDIS_PASSWORD: "${REDIS_PASSWORD:-fortress_redis_secret}"
      QSECBIT_DATA_DIR: /opt/hookprobe/fortress/data
      # Metrics export to Victoria
      VICTORIA_PUSH_URL: http://localhost:8428/api/v1/write
    secrets:
      - postgres_password
    volumes:
      - agent_data:/opt/hookprobe/fortress/data
      - fortress_config:/etc/hookprobe:ro
      - /sys/class/powercap:/sys/class/powercap:ro  # RAPL energy monitoring
    # Host network REQUIRED for traffic analysis on all interfaces
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    # Core service - always started (provides mesh threat detection)

  # LSTM Trainer - PyTorch model training (scheduled job)
  lstm-trainer:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.lstm
    image: localhost/fortress-lstm:latest
    container_name: fortress-lstm-trainer
    environment:
      LSTM_MODEL_DIR: /opt/hookprobe/fortress/data/ml-models/trained
      LSTM_CHECKPOINT_DIR: /opt/hookprobe/fortress/data/ml-models/checkpoints
    volumes:
      - ml_models:/opt/hookprobe/fortress/data/ml-models
      - agent_data:/opt/hookprobe/fortress/data:ro  # Read threat data
    networks:
      fortress-ml:
        ipv4_address: 172.20.202.10
    profiles:
      - training
    # One-shot job - doesn't restart
    restart: "no"

  # ============================================================
  # MANAGEMENT TIER - Monitoring (fortress-mgmt)
  # ============================================================

  # Grafana - Visualization Dashboard
  grafana:
    image: docker.io/grafana/grafana:10-alpine
    container_name: fortress-grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD_FILE: /run/secrets/grafana_password
      GF_INSTALL_PLUGINS: grafana-clock-panel
      GF_SERVER_ROOT_URL: "%(protocol)s://%(domain)s:%(http_port)s/grafana/"
    secrets:
      - grafana_password
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    ports:
      - "3000:3000"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      fortress-mgmt:
        ipv4_address: 172.20.203.10
    profiles:
      - monitoring

  # VictoriaMetrics - Time Series Database
  victoria:
    image: docker.io/victoriametrics/victoria-metrics:stable
    container_name: fortress-victoria
    restart: unless-stopped
    command:
      - "-storageDataPath=/storage"
      - "-retentionPeriod=30d"
      - "-httpListenAddr=:8428"
    volumes:
      - victoria_data:/storage
    ports:
      - "8428:8428"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8428/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      fortress-mgmt:
        ipv4_address: 172.20.203.11
    profiles:
      - monitoring

# ============================================================
# SECRETS (file-based for security)
# ============================================================
secrets:
  postgres_password:
    file: ./secrets/postgres_password
  flask_secret:
    file: ./secrets/flask_secret
  grafana_password:
    file: ./secrets/grafana_password

# ============================================================
# VOLUMES (persistent storage)
# ============================================================
volumes:
  # Data Tier
  postgres_data:
    driver: local
    name: fortress-postgres-data
  postgres_certs:
    driver: local
    name: fortress-postgres-certs
  redis_data:
    driver: local
    name: fortress-redis-data

  # Services Tier
  web_data:
    driver: local
    name: fortress-web-data
  web_logs:
    driver: local
    name: fortress-web-logs
  dnsxai_data:
    driver: local
    name: fortress-dnsxai-data
  dnsxai_blocklists:
    driver: local
    name: fortress-dnsxai-blocklists
  dfs_data:
    driver: local
    name: fortress-dfs-data

  # ML Tier
  agent_data:
    driver: local
    name: fortress-agent-data
  ml_models:
    driver: local
    name: fortress-ml-models

  # Management Tier
  grafana_data:
    driver: local
    name: fortress-grafana-data
  victoria_data:
    driver: local
    name: fortress-victoria-data

  # Shared
  fortress_config:
    driver: local
    name: fortress-config

# ============================================================
# NETWORKS - OVS-Integrated 4-Tier Security Architecture
# ============================================================
#
# These Podman bridge networks provide basic container connectivity.
# The OVS bridge (43ess) provides the security layer:
#
#   ┌─────────────────────────────────────────────────────────┐
#   │                  OVS Bridge: 43ess                      │
#   │  ┌─────────────┬─────────────┬──────────┬────────────┐ │
#   │  │43ess-data   │43ess-svc    │43ess-ml  │43ess-mgmt  │ │
#   │  │(internal)   │(internet OK)│(internal) │(internal)  │ │
#   │  └──────┬──────┴──────┬──────┴─────┬─────┴──────┬─────┘ │
#   │         │             │            │            │       │
#   │    ┌────┴────┐   ┌────┴────┐  ┌────┴────┐ ┌─────┴────┐ │
#   │    │postgres │   │  web    │  │  lstm   │ │ grafana  │ │
#   │    │redis    │   │dnsxai   │  │         │ │ victoria │ │
#   │    └─────────┘   │dfs      │  └─────────┘ └──────────┘ │
#   │                  └─────────┘                           │
#   │  ┌───────────────────────────────────────────────────┐ │
#   │  │ fortress-lan (10.200.0.0/23-29) - WiFi/LAN clients │ │
#   │  └───────────────────────────────────────────────────┘ │
#   │                                                        │
#   │  OpenFlow Rules:                                       │
#   │  - Table 0:  Ingress classification (ARP, DHCP, DNS)   │
#   │  - Table 10: Tier isolation (drop cross-tier traffic)  │
#   │  - Table 20: Internet control (allow/deny per tier)    │
#   │  - Table 30: Mirror (copy to QSecBit)                  │
#   │  - Table 40: Output (L2 forwarding)                    │
#   └─────────────────────────────────────────────────────────┘
#
networks:
  # Tier 1: DATA - Most sensitive (database, cache)
  # OVS port: fortress-data (172.20.200.1/24)
  # OpenFlow: NO internet, only web can reach this tier
  fortress-data:
    driver: bridge
    internal: true  # NO internet access
    ipam:
      config:
        - subnet: 172.20.200.0/24
          gateway: 172.20.200.1
    driver_opts:
      mtu: "9000"  # Jumbo frames for DB traffic
      # Note: Docker's enable_icc is not needed - Podman enables ICC by default

  # Tier 2: SERVICES - Web, DNS, APIs
  # OVS port: fortress-services (172.20.201.1/24)
  # OpenFlow: Internet OK (dnsXai needs upstream DNS)
  fortress-services:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.201.0/24
          gateway: 172.20.201.1
    driver_opts:
      mtu: "1500"

  # Tier 3: ML - Threat detection, inference
  # OVS port: fortress-ml (172.20.202.1/24)
  # OpenFlow: NO internet, QSecBit uses host network
  fortress-ml:
    driver: bridge
    internal: true  # NO internet access
    ipam:
      config:
        - subnet: 172.20.202.0/24
          gateway: 172.20.202.1

  # Tier 4: MANAGEMENT - Monitoring, metrics
  # OVS port: fortress-mgmt (172.20.203.1/24)
  # OpenFlow: NO internet, web can query metrics
  fortress-mgmt:
    driver: bridge
    internal: true  # NO internet access
    ipam:
      config:
        - subnet: 172.20.203.0/24
          gateway: 172.20.203.1
