# HookProbe Fortress - Container Orchestration with OVS
#
# Self-contained, modular deployment with OVS-based network security.
# All services run in isolation with OpenFlow-controlled traffic.
#
# Architecture:
#   - OVS bridge (FTS) provides OpenFlow-based tier isolation
#   - Podman bridge networks for container-to-container communication
#   - veth pairs connect containers to OVS for flow monitoring
#   - sFlow/IPFIX export to QSecBit for ML analysis
#   - Traffic mirroring captures all packets for threat detection
#
# Usage:
#   podman-compose up -d                       # Start core services
#   podman-compose --profile monitoring up -d  # Start with Grafana/Victoria
#   podman-compose --profile training run lstm # Run LSTM training job
#   podman-compose down                        # Stop all services
#   podman-compose logs -f web                 # Follow web container logs
#
# After startup, connect containers to OVS:
#   /opt/hookprobe/fortress/devices/common/ovs-container-network.sh connect-containers
#
# Container Groups:
#   - Core: postgres, redis, web (always running)
#   - Security: qsecbit-agent, dnsxai, dfs-intelligence (always running)
#   - Monitoring: grafana, victoria (--profile monitoring)
#   - Jobs: lstm-trainer (--profile training, one-shot)
#   - Mesh: mesh-orchestrator (--profile mesh) - HTP/Neuro/DSM communication
#
# Network (simplified for podman-compose 1.0.6 compatibility):
#   - fts-internal: 172.20.200.0/24 (all app containers share this network)
#   - IDS containers (suricata, zeek, xdp, qsecbit) use host network
#   - fts-lan: 10.200.0.0/23-29 (WiFi/LAN clients) - NAT to internet
#
# Network isolation is achieved via nftables rules on the host.
# Container IP assignments:
#   .10 postgres, .11 redis, .20 web, .21 dnsxai, .22 dfs
#   .30 grafana, .31 victoria, .40 lstm-trainer
#   .50 n8n, .51 clickhouse, .60 cloudflared, .70 mesh-orchestrator
#
# OVS Security Features:
#   - OpenFlow 1.3+ tier isolation rules
#   - Traffic mirroring to fts-mirror port
#   - sFlow export to 127.0.0.1:6343
#   - IPFIX export to 127.0.0.1:4739
#   - QoS meters for rate limiting
#   - VXLAN tunnels for mesh connectivity
#
# Version: 5.5.0
# License: AGPL-3.0

version: '3.8'

services:
  # ============================================================
  # DATA TIER - Isolated Database Layer (fts-data)
  # ============================================================

  # PostgreSQL Database - Most sensitive, fully isolated in data tier
  postgres:
    image: docker.io/library/postgres:15-alpine
    container_name: fts-postgres
    restart: unless-stopped
    hostname: fts-postgres
    environment:
      POSTGRES_DB: fortress
      POSTGRES_USER: fortress
      # Use environment variable directly (podman secrets have permission issues with non-root users)
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD:-fortress_db_secret}"
      # Enable SSL (certificates mounted from volume)
      PGSSLMODE: prefer
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
      - postgres_certs:/certs:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U fortress -d fortress"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      fts-internal:
        ipv4_address: 172.20.200.10

  # Redis Cache (Sessions + Rate Limiting)
  redis:
    image: docker.io/library/redis:7-alpine
    container_name: fts-redis
    restart: unless-stopped
    hostname: fts-redis
    command: >
      redis-server
      --appendonly yes
      --maxmemory 128mb
      --maxmemory-policy allkeys-lru
      --bind 0.0.0.0
      --requirepass "${REDIS_PASSWORD:-fortress_redis_secret}"
    environment:
      # Expose password to container for healthcheck
      REDIS_PASSWORD: "${REDIS_PASSWORD:-fortress_redis_secret}"
    volumes:
      - redis_data:/data
    healthcheck:
      # Simple PING check - redis-cli reads REDISCLI_AUTH env var
      test: ["CMD-SHELL", "REDISCLI_AUTH=\"$REDIS_PASSWORD\" redis-cli ping | grep -q PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    networks:
      fts-internal:
        ipv4_address: 172.20.200.11

  # ============================================================
  # SERVICES TIER - Web and DNS (fts-services)
  # ============================================================

  # Fortress Web Application (Flask + Gunicorn)
  # Services tier with access to data tier for database connectivity
  web:
    build:
      context: ..
      dockerfile: containers/Containerfile.web
    image: localhost/fts-web:latest
    container_name: fts-web
    restart: unless-stopped
    hostname: fts-web
    # Publish web port to host - required for rootless podman
    # VLAN clients access via host IP, not container IP
    ports:
      - "${WEB_PORT:-8443}:${WEB_PORT:-8443}"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      # Database connection via container network
      DATABASE_HOST: fts-postgres
      DATABASE_PORT: 5432
      DATABASE_NAME: fortress
      DATABASE_USER: fortress
      DATABASE_PASSWORD: "${POSTGRES_PASSWORD:-fortress_db_secret}"
      # Redis connection via container network
      REDIS_HOST: fts-redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: "${REDIS_PASSWORD:-fortress_redis_secret}"
      # Flask settings
      FLASK_ENV: production
      FLASK_SECRET_KEY: "${FLASK_SECRET_KEY:-fortress_flask_secret_key_change_me}"
      # Gunicorn settings
      GUNICORN_WORKERS: 2
      GUNICORN_THREADS: 4
      GUNICORN_BIND: "0.0.0.0:${WEB_PORT:-8443}"
      # ML service endpoints via container network
      QSECBIT_API_URL: http://localhost:9090
      DNSXAI_API_URL: http://fts-dnsxai:8080
      DFS_API_URL: http://fts-dfs:8050
      # Monitoring endpoints
      VICTORIA_URL: http://fts-victoria:8428
    volumes:
      - web_data:/app/data
      - web_logs:/app/logs
      # Share data from qsecbit agent (WAN health, QSecBit stats, SLAAI data)
      # Uses bind mount to match agent's bind mount (data visible on host)
      # NOTE: rw,U required - U suffix auto-adjusts ownership to container user
      # This fixes GID mismatch when host fortress group != container GID 1000
      - /opt/hookprobe/fortress/data:/opt/hookprobe/fortress/data:rw,U
      # Bind mount host config directory (install script creates users.json here)
      # Read-only, world-readable (644) - no secrets stored here
      - /etc/hookprobe:/etc/hookprobe:ro
      # SDN Auto Pilot database (device classification data)
      # NOTE: rw,U required for web UI to set policies (quarantine, move segment, etc.)
      - /var/lib/hookprobe:/var/lib/hookprobe:rw,U
    # Container network - accessible via VLAN 200 routing from management network
    # Access: MGMT VLAN (10.200.100.x) → veth bridge → Container (172.20.200.20:8443)
    networks:
      fts-internal:
        ipv4_address: 172.20.200.20
    # Health check - wait up to 90s for entrypoint (DB wait up to 60s + app startup)
    # Note: Uses WEB_PORT environment variable for dynamic port support
    healthcheck:
      test: ["CMD-SHELL", "curl -f -k https://localhost:${WEB_PORT:-8443}/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s

  # dnsXai Engine - DNS ML Protection
  # Provides both DNS filtering (5353/udp) and HTTP API (8080/tcp)
  dnsxai:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.dnsxai
    image: localhost/fts-dnsxai:latest
    container_name: fts-dnsxai
    restart: unless-stopped
    hostname: fts-dnsxai
    environment:
      DNSXAI_UPSTREAM: "1.1.1.1"
      DNSXAI_PROTECTION_LEVEL: 3
      # Path must match Containerfile: /opt/hookprobe/shared/dnsXai/data
      DNSXAI_DATA_DIR: /opt/hookprobe/shared/dnsXai/data
      # User data directory for persistent whitelist (survives reinstall AND purge)
      # CRITICAL: /etc/hookprobe/persistent/ is NEVER removed by any uninstall
      DNSXAI_USERDATA_DIR: /etc/hookprobe/persistent/dnsxai
      LOG_DIR: /var/log/hookprobe
    volumes:
      # Mount over shared path to persist data while keeping bundled whitelist
      - dnsxai_data:/opt/hookprobe/shared/dnsXai/data
      - dnsxai_blocklists:/opt/hookprobe/shared/dnsXai/blocklists
      # PERSISTENT USER DATA - NEVER removed, even with --purge
      # This location survives ALL uninstall operations
      - /etc/hookprobe/persistent/dnsxai:/etc/hookprobe/persistent/dnsxai:U
      # Legacy mount for backwards compatibility (may contain symlinks)
      - /var/lib/hookprobe/userdata/dnsxai:/opt/hookprobe/shared/dnsXai/userdata:U
      # Bind mount logs to host for ML training data visibility
      - /var/log/hookprobe:/var/log/hookprobe:U
      - /etc/hookprobe:/etc/hookprobe:ro
      # DoT (DNS over TLS) certificates - writable for auto-generation
      - dnsxai_certs:/etc/hookprobe/certs
    ports:
      - "127.0.0.1:53:5353/udp"  # DNS on localhost (dnsmasq forwards here) - frees 5353 for mDNS
      - "0.0.0.0:853:853/tcp"    # DoT (DNS over TLS) for Windows/modern clients
      - "127.0.0.1:8053:8080"    # HTTP API (localhost only for security)
    healthcheck:
      # Check HTTP API health endpoint using Python urllib (always available)
      test: ["CMD-SHELL", "python3 -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:8080/health\", timeout=5)'"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    networks:
      fts-internal:
        ipv4_address: 172.20.200.21
    # Core service - always started (provides mesh DNS protection)

  # DFS Intelligence - WiFi Channel ML
  dfs-intelligence:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.dfs
    image: localhost/fts-dfs:latest
    container_name: fts-dfs
    restart: unless-stopped
    hostname: fts-dfs
    environment:
      DFS_DATA_DIR: /opt/hookprobe/wireless/data
      DFS_DB_PATH: /opt/hookprobe/wireless/data/dfs_radar.db
    volumes:
      - dfs_data:/opt/hookprobe/wireless/data
      - /etc/hookprobe:/etc/hookprobe:ro
    ports:
      - "0.0.0.0:8050:8050"  # DFS API accessible from LAN
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8050/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      fts-internal:
        ipv4_address: 172.20.200.22
    # Core service - always started (provides WiFi DFS intelligence)

  # ============================================================
  # SECURITY TIER - Threat Detection (fts-ml)
  # ============================================================

  # QSecBit Agent - Threat Detection (numpy, scipy, sklearn)
  # Uses host network for traffic capture on all interfaces
  qsecbit-agent:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.agent
    image: localhost/fts-agent:latest
    container_name: fts-qsecbit
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      QSECBIT_MODE: fortress
      # Database via container network gateway (host can reach container IPs)
      DATABASE_HOST: 172.20.200.10
      DATABASE_PORT: 5432
      DATABASE_NAME: fortress
      DATABASE_USER: fortress
      DATABASE_PASSWORD: "${POSTGRES_PASSWORD:-fortress_db_secret}"
      REDIS_HOST: 172.20.200.11
      REDIS_PORT: 6379
      REDIS_PASSWORD: "${REDIS_PASSWORD:-fortress_redis_secret}"
      QSECBIT_DATA_DIR: /opt/hookprobe/fortress/data
      # Metrics export to Victoria (mgmt tier)
      VICTORIA_PUSH_URL: http://172.20.200.31:8428/api/v1/write
    volumes:
      # Bind mount for shared data (visible on host and other containers)
      # U suffix auto-adjusts ownership to container user (fixes GID mismatch)
      - /opt/hookprobe/fortress/data:/opt/hookprobe/fortress/data:U
      - /etc/hookprobe:/etc/hookprobe:ro
      - /etc/hostapd:/etc/hostapd:ro  # Read hostapd config for WiFi SSID/channel
      - /sys/class/powercap:/sys/class/powercap:ro  # RAPL energy monitoring
      - /run/hostapd:/run/hostapd:ro  # Access hostapd control socket
      - /run/fortress:/run/fortress:ro  # PBR failover state file
      - /var/lib/fortress:/var/lib/fortress:ro  # PBR JSON state file
      - /var/lib/misc:/var/lib/misc:ro  # dnsmasq leases for DHCP hostname lookup
      - /run/avahi-daemon:/run/avahi-daemon:ro  # Avahi socket for mDNS discovery
    # Host network REQUIRED for traffic analysis on all interfaces
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    # Health check - verify HTTP API is responding on port 9090
    healthcheck:
      test: ["CMD-SHELL", "python3 -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:9090/health\", timeout=5)'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    # Core service - always started (provides mesh threat detection)

  # Ecosystem Bubble Manager - Same-user device detection (mDNS, ML clustering)
  # Uses host network for mDNS multicast (224.0.0.251:5353) and OVS access
  bubble-manager:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.bubble
    image: localhost/fts-bubble:latest
    container_name: fts-bubble-manager
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      FORTRESS_CONFIG: /etc/hookprobe/fortress.conf
      PYTHONPATH: /opt/hookprobe/fortress/lib:/opt/hookprobe/fortress:/opt/hookprobe
      # Database access for device identity
      DATABASE_HOST: 172.20.200.10
      DATABASE_PORT: 5432
      DATABASE_NAME: fortress
      DATABASE_USER: fortress
      DATABASE_PASSWORD: "${POSTGRES_PASSWORD:-fortress_db_secret}"
    volumes:
      # Data directories
      - /var/lib/hookprobe:/var/lib/hookprobe:U
      - /var/lib/fortress:/var/lib/fortress:U
      - /opt/hookprobe/fortress/data:/opt/hookprobe/fortress/data:U
      # Configuration
      - /etc/hookprobe:/etc/hookprobe:ro
      # OVS socket for OpenFlow commands (ovs-ofctl, ovs-vsctl)
      - /run/openvswitch:/run/openvswitch
      # Zeek logs for D2D connection analysis (connection_graph.py)
      - zeek_logs:/var/log/zeek:ro
    # Host network REQUIRED for:
    # - mDNS multicast discovery (224.0.0.251:5353)
    # - OVS OpenFlow rule application
    network_mode: host
    cap_add:
      - NET_ADMIN  # Required for OVS OpenFlow commands
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'ecosystem_bubble.py' > /dev/null"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    # Core service - always started (enables Apple ecosystem, smart home grouping)

  # LSTM Trainer - PyTorch model training (scheduled job)
  lstm-trainer:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.lstm
    image: localhost/fts-lstm:latest
    container_name: fts-lstm-trainer
    environment:
      LSTM_MODEL_DIR: /opt/hookprobe/fortress/data/ml-models/trained
      LSTM_CHECKPOINT_DIR: /opt/hookprobe/fortress/data/ml-models/checkpoints
    volumes:
      - ml_models:/opt/hookprobe/fortress/data/ml-models
      - /opt/hookprobe/fortress/data:/opt/hookprobe/fortress/data:ro  # Read threat data
    networks:
      fts-internal:
        ipv4_address: 172.20.200.40
    profiles:
      - training
    # One-shot job - doesn't restart
    restart: "no"

  # ============================================================
  # IDS/IPS TIER - Network Security Monitoring (host network)
  # ============================================================

  # Suricata IDS/IPS - Deep Packet Inspection and Threat Detection
  # Uses host network for direct packet capture on all interfaces
  suricata:
    image: docker.io/jasonish/suricata:latest
    container_name: fts-suricata
    restart: unless-stopped
    hostname: fts-suricata
    # The jasonish/suricata image entrypoint handles config loading
    # -i <interface> specifies the capture interface
    # --af-packet uses AF_PACKET for efficient capture
    command:
      - -c
      - /etc/suricata/suricata.yaml
      - -i
      - "${SURICATA_INTERFACE:-FTS}"
      - --af-packet
    environment:
      # Interface to monitor (set during install based on detection)
      SURICATA_INTERFACE: "${SURICATA_INTERFACE:-FTS}"
    volumes:
      - suricata_logs:/var/log/suricata
      - suricata_rules:/var/lib/suricata
      # Don't mount config volume - use the default config from the image
      # - suricata_config:/etc/suricata
      - /etc/hookprobe:/etc/hookprobe:ro
    # Host network REQUIRED for packet capture on bridge/physical interfaces
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
      - SYS_NICE
    # Core IDS service - always started
    healthcheck:
      test: ["CMD", "pgrep", "-x", "suricata"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 60s
    profiles:
      - ids

  # Zeek Network Security Monitor - Protocol Analysis and Threat Detection
  # Analyzes HTTP, DNS, SSL/TLS, SSH, and other protocols
  zeek:
    image: docker.io/zeek/zeek:latest
    container_name: fts-zeek
    restart: unless-stopped
    hostname: fts-zeek
    environment:
      # Interface to monitor - OVS bridge (FTS) or WAN interface
      ZEEK_INTERFACE: "${ZEEK_INTERFACE:-FTS}"
    command:
      - zeek
      - -C                                    # Ignore checksums (fixes NIC offloading warnings)
      - -i
      - ${ZEEK_INTERFACE:-FTS}
      - local
      - LogAscii::use_json=T
      - "Site::local_nets={10.200.0.0/16, 172.20.0.0/16, 192.168.0.0/16}"
    volumes:
      - zeek_logs:/usr/local/zeek/logs
      - zeek_spool:/usr/local/zeek/spool
      - zeek_config:/usr/local/zeek/share/zeek/site
      - /etc/hookprobe:/etc/hookprobe:ro
    # Host network REQUIRED for packet capture
    network_mode: host
    # Privileged required for rootless podman packet capture
    privileged: true
    cap_add:
      - NET_ADMIN
      - NET_RAW
    # Core network analysis - always started
    healthcheck:
      test: ["CMD", "pgrep", "-x", "zeek"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    profiles:
      - ids

  # XDP/eBPF DDoS Protection - Kernel-level packet filtering
  # Provides high-performance DDoS mitigation at line rate
  xdp-protection:
    build:
      context: ../..
      dockerfile: products/fortress/containers/Containerfile.xdp
    image: localhost/fts-xdp:latest
    container_name: fts-xdp
    restart: unless-stopped
    hostname: fts-xdp
    environment:
      XDP_INTERFACE: "${XDP_INTERFACE:-FTS}"
      # Rate limits (packets per second)
      XDP_SYN_LIMIT: 1000
      XDP_UDP_LIMIT: 5000
      XDP_ICMP_LIMIT: 100
    volumes:
      - xdp_data:/opt/hookprobe/xdp/data
      - /sys/fs/bpf:/sys/fs/bpf
      - /etc/hookprobe:/etc/hookprobe:ro
    # Host network and privileged for eBPF
    network_mode: host
    privileged: true
    cap_add:
      - NET_ADMIN
      - SYS_ADMIN
      - BPF
    healthcheck:
      test: ["CMD", "bpftool", "prog", "list"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 20s
    profiles:
      - ids

  # ============================================================
  # MANAGEMENT TIER - Monitoring (fts-mgmt)
  # ============================================================

  # Grafana - Visualization Dashboard
  grafana:
    image: docker.io/grafana/grafana:11.4.0
    container_name: fts-grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: "${GRAFANA_PASSWORD:-fortress_grafana_admin}"
      # Disable plugin installation - requires internet which may not be available in container network
      # GF_INSTALL_PLUGINS: grafana-clock-panel
      GF_SERVER_ROOT_URL: "%(protocol)s://%(domain)s:%(http_port)s/grafana/"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    ports:
      - "0.0.0.0:3000:3000"  # Grafana accessible from LAN
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      fts-internal:
        ipv4_address: 172.20.200.30
    profiles:
      - monitoring

  # VictoriaMetrics - Time Series Database
  victoria:
    image: docker.io/victoriametrics/victoria-metrics:v1.106.1
    container_name: fts-victoria
    restart: unless-stopped
    command:
      - "-storageDataPath=/storage"
      - "-retentionPeriod=30d"
      - "-httpListenAddr=:8428"
    volumes:
      - victoria_data:/storage
    ports:
      - "0.0.0.0:8428:8428"  # VictoriaMetrics accessible from LAN
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8428/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      fts-internal:
        ipv4_address: 172.20.200.31
    profiles:
      - monitoring

  # ============================================================
  # AUTOMATION TIER - Workflow and Analytics (optional)
  # ============================================================

  # n8n - Workflow Automation
  # Enables automated security response workflows, notifications, integrations
  n8n:
    image: docker.io/n8nio/n8n:latest
    container_name: fts-n8n
    restart: unless-stopped
    hostname: fts-n8n
    environment:
      N8N_HOST: 0.0.0.0
      N8N_PORT: 5678
      N8N_PROTOCOL: http
      WEBHOOK_URL: "http://fts-n8n:5678/"
      # Database connection (use postgres for persistence)
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: fts-postgres
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_DATABASE: fortress
      DB_POSTGRESDB_USER: fortress
      DB_POSTGRESDB_PASSWORD: "${POSTGRES_PASSWORD:-fortress_db_secret}"
      DB_POSTGRESDB_SCHEMA: n8n
      # Timezone
      GENERIC_TIMEZONE: UTC
      TZ: UTC
      # Basic auth for security
      N8N_BASIC_AUTH_ACTIVE: "true"
      N8N_BASIC_AUTH_USER: "${N8N_USER:-admin}"
      N8N_BASIC_AUTH_PASSWORD: "${N8N_PASSWORD:-fortress_n8n_admin}"
    volumes:
      - n8n_data:/home/node/.n8n
      - /etc/hookprobe:/etc/hookprobe:ro
    ports:
      - "0.0.0.0:5678:5678"  # n8n web UI
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:5678/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      fts-internal:
        ipv4_address: 172.20.200.50
    depends_on:
      postgres:
        condition: service_healthy
    profiles:
      - n8n
      - automation

  # ClickHouse - Analytics Database
  # High-performance columnar database for security event analytics
  clickhouse:
    image: docker.io/clickhouse/clickhouse-server:latest
    container_name: fts-clickhouse
    restart: unless-stopped
    hostname: fts-clickhouse
    environment:
      CLICKHOUSE_DB: fortress
      CLICKHOUSE_USER: fortress
      CLICKHOUSE_PASSWORD: "${CLICKHOUSE_PASSWORD:-fortress_clickhouse_secret}"
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_logs:/var/log/clickhouse-server
      - /etc/hookprobe:/etc/hookprobe:ro
    ports:
      - "0.0.0.0:8123:8123"  # HTTP API
      - "0.0.0.0:9000:9000"  # Native client
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8123/ping"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      fts-internal:
        ipv4_address: 172.20.200.51
    profiles:
      - clickhouse
      - analytics

  # Cloudflare Tunnel - Remote Access
  # Secure tunnel for remote admin access without exposing ports
  cloudflared:
    image: docker.io/cloudflare/cloudflared:latest
    container_name: fts-cloudflared
    restart: unless-stopped
    hostname: fts-cloudflared
    command: tunnel --no-autoupdate run --token ${CLOUDFLARE_TOKEN:-}
    environment:
      TUNNEL_TOKEN: "${CLOUDFLARE_TOKEN:-}"
    networks:
      fts-internal:
        ipv4_address: 172.20.200.60
    profiles:
      - tunnel
      - remote-access

  # ============================================================
  # MESH TIER - Inter-Product Communication (HTP/Neuro/DSM)
  # ============================================================
  # MOVED TO: podman-compose.mesh.yml
  #
  # Mesh orchestrator is in a separate file because podman-compose 1.x
  # doesn't properly support profiles. To enable mesh federation:
  #
  #   podman-compose -f podman-compose.yml -f podman-compose.mesh.yml up -d
  #
  # Or build and start mesh only:
  #   podman build -f Containerfile.mesh -t fts-mesh:latest ../..
  #   podman-compose -f podman-compose.yml -f podman-compose.mesh.yml up -d mesh-orchestrator
  # ============================================================

# ============================================================
# SECRETS
# ============================================================
# NOTE: File-based secrets disabled due to podman permission issues with non-root users.
# Secrets are now passed via environment variables. For production, consider:
# - Using podman secret with proper UID mapping
# - Or using a secrets management solution like HashiCorp Vault
#
# secrets:
#   postgres_password:
#     file: ./secrets/postgres_password

# ============================================================
# VOLUMES (persistent storage)
# ============================================================
volumes:
  # Data Tier
  postgres_data:
    driver: local
    name: fts-postgres-data
  postgres_certs:
    driver: local
    name: fts-postgres-certs
  redis_data:
    driver: local
    name: fts-redis-data

  # Services Tier
  web_data:
    driver: local
    name: fts-web-data
  web_logs:
    driver: local
    name: fts-web-logs
  dnsxai_data:
    driver: local
    name: fts-dnsxai-data
  dnsxai_blocklists:
    driver: local
    name: fts-dnsxai-blocklists
  dnsxai_logs:
    driver: local
    name: fts-dnsxai-logs
  dnsxai_certs:
    driver: local
    name: fts-dnsxai-certs
  dfs_data:
    driver: local
    name: fts-dfs-data

  # ML Tier
  # Note: agent_data is now a bind mount to /opt/hookprobe/fortress/data
  # This allows host visibility and sharing between containers
  ml_models:
    driver: local
    name: fts-ml-models

  # Management Tier
  grafana_data:
    driver: local
    name: fts-grafana-data
  victoria_data:
    driver: local
    name: fts-victoria-data

  # IDS/IPS Tier
  suricata_logs:
    driver: local
    name: fts-suricata-logs
  suricata_rules:
    driver: local
    name: fts-suricata-rules
  # suricata_config not used - image provides default config
  zeek_logs:
    driver: local
    name: fts-zeek-logs
  zeek_spool:
    driver: local
    name: fts-zeek-spool
  zeek_config:
    driver: local
    name: fts-zeek-config
  xdp_data:
    driver: local
    name: fts-xdp-data

  # Automation Tier
  n8n_data:
    driver: local
    name: fts-n8n-data
  clickhouse_data:
    driver: local
    name: fts-clickhouse-data
  clickhouse_logs:
    driver: local
    name: fts-clickhouse-logs

  # Mesh Tier volumes are in podman-compose.mesh.yml

  # Note: /etc/hookprobe is bind-mounted from host, not a named volume
  # This allows the install script to create users.json in the host directory

# ============================================================
# NETWORKS - OVS-Integrated 4-Tier Security Architecture
# ============================================================
#
# These Podman bridge networks provide basic container connectivity.
# The OVS bridge (FTS) provides the security layer:
#
#   ┌─────────────────────────────────────────────────────────┐
#   │                  OVS Bridge: FTS                      │
#   │  ┌─────────────┬─────────────┬──────────┬────────────┐ │
#   │  │FTS-data   │FTS-svc    │FTS-ml  │FTS-mgmt  │ │
#   │  │(internal)   │(internet OK)│(internal) │(internal)  │ │
#   │  └──────┬──────┴──────┬──────┴─────┬─────┴──────┬─────┘ │
#   │         │             │            │            │       │
#   │    ┌────┴────┐   ┌────┴────┐  ┌────┴────┐ ┌─────┴────┐ │
#   │    │postgres │   │  web    │  │  lstm   │ │ grafana  │ │
#   │    │redis    │   │dnsxai   │  │         │ │ victoria │ │
#   │    └─────────┘   │dfs      │  └─────────┘ └──────────┘ │
#   │                  └─────────┘                           │
#   │  ┌───────────────────────────────────────────────────┐ │
#   │  │ fts-lan (10.200.0.0/23-29) - WiFi/LAN clients │ │
#   │  └───────────────────────────────────────────────────┘ │
#   │                                                        │
#   │  OpenFlow Rules:                                       │
#   │  - Table 0:  Ingress classification (ARP, DHCP, DNS)   │
#   │  - Table 10: Tier isolation (drop cross-tier traffic)  │
#   │  - Table 20: Internet control (allow/deny per tier)    │
#   │  - Table 30: Mirror (copy to QSecBit)                  │
#   │  - Table 40: Output (L2 forwarding)                    │
#   └─────────────────────────────────────────────────────────┘
#
networks:
  # SIMPLIFIED NETWORK - Single internal network for podman-compose 1.0.6 compatibility
  # podman-compose 1.0.6 cannot handle --ip with multiple networks, so we use ONE network.
  #
  # Network isolation is achieved via:
  # - nftables rules on the host (setup by install script)
  # - OVS OpenFlow rules (if using OVS mode)
  #
  # All application containers share this network and can reach each other via DNS names.
  # IDS containers (suricata, zeek, xdp, qsecbit) use host network for traffic capture.
  fts-internal:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.200.0/24
          gateway: 172.20.200.1
    driver_opts:
      mtu: "1500"

  # Legacy network names kept for potential manual multi-network setup
  # These are NOT used by default due to podman-compose 1.0.6 limitations
  # fts-data, fts-services, fts-ml, fts-mgmt - see comments above
